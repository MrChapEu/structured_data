{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize\n",
    "#from keras.applications.imagenet_utils import preprocess_input\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data  = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.92106336],\n",
       "         [ 0.92106336],\n",
       "         [ 0.92106336],\n",
       "         ..., \n",
       "         [ 0.85965914],\n",
       "         [ 0.92106336],\n",
       "         [ 0.92106336]],\n",
       "\n",
       "        [[ 0.85965914],\n",
       "         [ 0.85965914],\n",
       "         [ 0.85965914],\n",
       "         ..., \n",
       "         [ 0.85965914],\n",
       "         [ 0.73685068],\n",
       "         [ 0.73685068]],\n",
       "\n",
       "        [[ 0.85965914],\n",
       "         [ 0.85965914],\n",
       "         [ 0.85965914],\n",
       "         ..., \n",
       "         [ 0.73685068],\n",
       "         [ 0.61404222],\n",
       "         [ 0.61404222]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.67544645],\n",
       "         [ 0.73685068],\n",
       "         [ 0.98246759],\n",
       "         ..., \n",
       "         [ 0.85965914],\n",
       "         [ 0.85965914],\n",
       "         [ 0.85965914]],\n",
       "\n",
       "        [[ 0.55263799],\n",
       "         [ 0.55263799],\n",
       "         [ 0.67544645],\n",
       "         ..., \n",
       "         [ 0.73685068],\n",
       "         [ 0.73685068],\n",
       "         [ 0.73685068]],\n",
       "\n",
       "        [[ 1.04387188],\n",
       "         [ 1.04387188],\n",
       "         [ 1.04387188],\n",
       "         ..., \n",
       "         [ 0.79825491],\n",
       "         [ 0.79825491],\n",
       "         [ 0.79825491]]],\n",
       "\n",
       "\n",
       "       [[[ 3.215312  ],\n",
       "         [ 3.215312  ],\n",
       "         [ 3.23138857],\n",
       "         ..., \n",
       "         [ 3.2635417 ],\n",
       "         [ 3.23138857],\n",
       "         [ 3.215312  ]],\n",
       "\n",
       "        [[ 3.23138857],\n",
       "         [ 3.23138857],\n",
       "         [ 3.23138857],\n",
       "         ..., \n",
       "         [ 3.16708231],\n",
       "         [ 3.19923544],\n",
       "         [ 3.23138857]],\n",
       "\n",
       "        [[ 3.24746513],\n",
       "         [ 3.24746513],\n",
       "         [ 3.24746513],\n",
       "         ..., \n",
       "         [ 3.18315887],\n",
       "         [ 3.29569483],\n",
       "         [ 3.31177139]],\n",
       "\n",
       "        ..., \n",
       "        [[ 3.2635417 ],\n",
       "         [ 3.23138857],\n",
       "         [ 3.29569483],\n",
       "         ..., \n",
       "         [ 3.31177139],\n",
       "         [ 3.13492918],\n",
       "         [ 3.13492918]],\n",
       "\n",
       "        [[ 3.2635417 ],\n",
       "         [ 3.29569483],\n",
       "         [ 3.07062292],\n",
       "         ..., \n",
       "         [ 3.16708231],\n",
       "         [ 3.23138857],\n",
       "         [ 3.23138857]],\n",
       "\n",
       "        [[ 3.08669949],\n",
       "         [ 3.32784796],\n",
       "         [ 3.11885262],\n",
       "         ..., \n",
       "         [ 3.24746513],\n",
       "         [ 3.27961826],\n",
       "         [ 3.32784796]]],\n",
       "\n",
       "\n",
       "       [[[ 2.80675626],\n",
       "         [ 2.83348727],\n",
       "         [ 2.72656322],\n",
       "         ..., \n",
       "         [ 2.80675626],\n",
       "         [ 2.80675626],\n",
       "         [ 2.78002524]],\n",
       "\n",
       "        [[ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         [ 2.72656322],\n",
       "         ..., \n",
       "         [ 2.80675626],\n",
       "         [ 2.80675626],\n",
       "         [ 2.78002524]],\n",
       "\n",
       "        [[ 2.78002524],\n",
       "         [ 2.72656322],\n",
       "         [ 2.75329423],\n",
       "         ..., \n",
       "         [ 2.80675626],\n",
       "         [ 2.80675626],\n",
       "         [ 2.78002524]],\n",
       "\n",
       "        ..., \n",
       "        [[ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         ..., \n",
       "         [ 2.96714234],\n",
       "         [ 2.8869493 ],\n",
       "         [ 2.80675626]],\n",
       "\n",
       "        [[ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         ..., \n",
       "         [ 2.78002524],\n",
       "         [ 2.56617713],\n",
       "         [ 2.78002524]],\n",
       "\n",
       "        [[ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         [ 2.78002524],\n",
       "         ..., \n",
       "         [ 2.80675626],\n",
       "         [ 2.80675626],\n",
       "         [ 2.78002524]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 3.21939135],\n",
       "         [ 3.31844974],\n",
       "         [ 3.51656604],\n",
       "         ..., \n",
       "         [ 4.20997334],\n",
       "         [ 4.16044426],\n",
       "         [ 4.11091518]],\n",
       "\n",
       "        [[ 3.31844974],\n",
       "         [ 3.31844974],\n",
       "         [ 3.46703696],\n",
       "         ..., \n",
       "         [ 4.20997334],\n",
       "         [ 4.16044426],\n",
       "         [ 4.16044426]],\n",
       "\n",
       "        [[ 3.41750789],\n",
       "         [ 3.26892042],\n",
       "         [ 3.41750789],\n",
       "         ..., \n",
       "         [ 4.25950241],\n",
       "         [ 4.20997334],\n",
       "         [ 4.20997334]],\n",
       "\n",
       "        ..., \n",
       "        [[ 3.26892042],\n",
       "         [ 3.51656604],\n",
       "         [ 3.31844974],\n",
       "         ..., \n",
       "         [ 4.55667734],\n",
       "         [ 4.60620642],\n",
       "         [ 4.55667734]],\n",
       "\n",
       "        [[ 3.16986227],\n",
       "         [ 3.56609511],\n",
       "         [ 3.56609511],\n",
       "         ..., \n",
       "         [ 4.60620642],\n",
       "         [ 4.55667734],\n",
       "         [ 4.55667734]],\n",
       "\n",
       "        [[ 3.16986227],\n",
       "         [ 3.61562419],\n",
       "         [ 3.66515326],\n",
       "         ..., \n",
       "         [ 4.60620642],\n",
       "         [ 4.55667734],\n",
       "         [ 4.55667734]]],\n",
       "\n",
       "\n",
       "       [[[ 3.11480784],\n",
       "         [ 3.04005241],\n",
       "         [ 3.04005241],\n",
       "         ..., \n",
       "         [ 3.04005241],\n",
       "         [ 3.0151341 ],\n",
       "         [ 3.0151341 ]],\n",
       "\n",
       "        [[ 2.94037867],\n",
       "         [ 3.08988953],\n",
       "         [ 3.11480784],\n",
       "         ..., \n",
       "         [ 3.04005241],\n",
       "         [ 3.0151341 ],\n",
       "         [ 3.0151341 ]],\n",
       "\n",
       "        [[ 3.1397264 ],\n",
       "         [ 3.08988953],\n",
       "         [ 3.04005241],\n",
       "         ..., \n",
       "         [ 3.04005241],\n",
       "         [ 3.04005241],\n",
       "         [ 3.0151341 ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 3.06497097],\n",
       "         [ 3.06497097],\n",
       "         [ 3.04005241],\n",
       "         ..., \n",
       "         [ 3.06497097],\n",
       "         [ 3.1397264 ],\n",
       "         [ 3.04005241]],\n",
       "\n",
       "        [[ 3.08988953],\n",
       "         [ 3.06497097],\n",
       "         [ 3.04005241],\n",
       "         ..., \n",
       "         [ 2.96529698],\n",
       "         [ 2.96529698],\n",
       "         [ 3.06497097]],\n",
       "\n",
       "        [[ 3.06497097],\n",
       "         [ 3.04005241],\n",
       "         [ 3.0151341 ],\n",
       "         ..., \n",
       "         [ 2.96529698],\n",
       "         [ 2.99021554],\n",
       "         [ 3.1397264 ]]],\n",
       "\n",
       "\n",
       "       [[[ 5.19128227],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605],\n",
       "         ..., \n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916]],\n",
       "\n",
       "        [[ 5.19128227],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605],\n",
       "         ..., \n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916]],\n",
       "\n",
       "        [[ 5.19128227],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605],\n",
       "         ..., \n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916]],\n",
       "\n",
       "        ..., \n",
       "        [[ 5.30537605],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605],\n",
       "         ..., \n",
       "         [ 5.30537605],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605]],\n",
       "\n",
       "        [[ 5.30537605],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605],\n",
       "         ..., \n",
       "         [ 5.30537605],\n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916]],\n",
       "\n",
       "        [[ 5.30537605],\n",
       "         [ 5.24832916],\n",
       "         [ 5.30537605],\n",
       "         ..., \n",
       "         [ 5.36242342],\n",
       "         [ 5.24832916],\n",
       "         [ 5.24832916]]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[()][\"X\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a batch of images\n",
    "\n",
    "The predict_batch function is defined as follows:\n",
    "\n",
    "    - open each image, and resize them to img_size\n",
    "    - stack them as a batch tensor of shape (batch, img_size_x, img_size_y)\n",
    "    - preprocess the batch and make a forward pass with the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def predict_batch(model, img_batch_path, img_size=None):\n",
    "def predict_batch(folder, img_size=None):\n",
    "    img_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        try:\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                img = imread(os.path.join(folder,filename))\n",
    "                lb = filename.split('_')[1]\n",
    "                lb = lb.lower()\n",
    "\n",
    "            if img_size:\n",
    "                img = imresize(img,img_size)\n",
    "\n",
    "            img = img.astype('float32')\n",
    "\n",
    "            # convert image to greyscale\n",
    "            img = img.sum(axis=2) / 3.\n",
    "            img /= np.std(img)\n",
    "\n",
    "            img = img[:, :, np.newaxis]\n",
    "\n",
    "            img_list.append(img)\n",
    "            label_list.append(lb)\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        raise ValueError(\n",
    "            'when both img_size and crop_size are None, all images '\n",
    "            'in image_paths must have the same shapes.')\n",
    "\n",
    "    #batch = preprocess_input(img_batch)\n",
    "    return img_batch, label_list \n",
    "            #model.predict(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (10000, 32, 100, 1)\n",
      "label shape 10000\n"
     ]
    }
   ],
   "source": [
    "YOUR_FOLDER_NAME = \"Synth90k\"\n",
    "\n",
    "output, labels = predict_batch(YOUR_FOLDER_NAME, (32, 100))\n",
    "print(\"output shape\", output.shape)\n",
    "print ('label shape', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 100, 1)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output[0][:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x147691470>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACOCAYAAAAhHfOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG9JJREFUeJztnV+sXFd1xr/lawcnNqROCk5iu7ZFLiQWKFBFaVqqKiIg\npS1q+kRBoooqKksVVaGiKilvPFSiUoXah6qSBbSpikARIBEhaBu5QS0SopBCnGCb2mC7tmPHkIQk\nlKSO3dWHGY+/Wdy17r5n5s41Z77fi8+559/e65zZPvs764+5O4QQQvzss26tGyCEEGI6aEAXQoie\noAFdCCF6ggZ0IYToCRrQhRCiJ2hAF0KInqABXQghesJEA7qZ3WNm3zWzo2Z2/7QaJYQQYuVY18Ai\nM1sA8F8A3g7gFIBvAHi3ux+cXvOEEEK0Mskb+h0Ajrr79939PIDPALh3Os0SQgixUtZPcOw2ACdp\n/RSAXyovtn69b9iwYdkTr2Y6gmmce7XTJZjZRPu1Hr8Ssj632qLar9rGfan61drnSfuxkuu2tr0V\nbmO2XB3T1X6z6sc0rtVqi7hfl/s/6e80bqv2e+mll37o7q9e7lqTDOhNmNleAHsBYP369di1a9ey\nx1y4cGHV2jONc7/88stTaElOy396wMCeK/n7JGR2a7VnZbNqG9uisktrn7P2dr2nrW3KliOVPXkb\nL7fatmprl34stb5U+6ptcb9JbdZ63WizLve/9Xfa1bbM4cOHT7Rca5Jf/mkAO2h9+/BvY7j7PgD7\nAGDjxo2+moP1lQ7f2M2bN4+Wr7rqqvSY8+fPp9tabTnpYDwN4kPND++WLVtGy5UtuhDtl/3444+p\nsg3/+KcxmGbHRPgcXQbZVloHRQB46aWXJrpWdf5pPJ+ZnTZu3Jjux89gtGV1H/necdtbn8EXXngh\nPXcrk2jo3wCwaGa7zewqAO8C8NDELRJCCNGJzv+Vu/sFM/tDAP8MYAHAJ939O1NrmRBCiBUx0dzM\n3b8E4EtTaosQQogJWPWPon2kVQ+NzFLbbN02KVWfWvvLOmylybbq2kzXD8yVRh3115US+9H6obZV\nQ2dav89U2nX1zaD6DsFkH7dbv110/aiena/Sxqv2TfrxPW7jtr/44otN565Q6L8QQvQEDehCCNET\neiu5XInukdymZ599duLzdZV+LjENuaTr+bu4UnaddjNdXP9aXSm7uphm7bj66qubzledu4tbYZSU\neD3aKXPxe9WrXjW2H9udzxefYV7n/sf9WJ74yU9+suTfAeD5558fLbNbIB8DjNuJt1XyU0XWj7jt\nla985Wh569at6fmeeeaZpuvqDV0IIXqCBnQhhOgJM5Vc3P2KlEIusRoh80z2dbsKf2bitDOLRJx2\n5CAw/dD/bAre1YNk2s9VJWm0yh2Zd0g8hqfkvBxtwbJA5RGRyQLXXHPN2DqfnyWSOPW//vrrR8sc\n1QsA11133Wj52muvHS1HmYGvVUlYLEHwMp87wv2tpBSWX5566qmx/c6dO7fktrNnz6b7VfJORSZN\nVRJbK3pDF0KInqABXQgheoIGdCGE6Am9dVvMWG2dvPXa0a1r0vN1YSW6c5d0rdW1Mr2x+p5QfSdg\nWlPuMitJnzpp+uRpRBtWtmDtmXXzqI2z/v2a17xmtLx79+6x/fg4PiauVxowt7eKjszuV2sfW+93\nvC6vs4Z+/Pjxsf1426lTp8a28b7sZhhdDvnZZ00+6v9d0Bu6EEL0BA3oQgjRE2butjjpdHXS6Mi1\npEuFoWkk3WpNUFRt43UuzhHJIgcrl0uegsa2Zm5drcmVIpkrYSwuwFGfrdGW1XWraEvuM18r2qLV\nzTCTUrZv3z62H6/zMVEOZHkjwm18+umnR8sxEpplB77fUY5g2aEaKzJ3yehWecMNNyx5TOwj2/am\nm24aLce+33zzzaPl06fH6/lwNTbeduTIkbH9eBtLOPEZ7DJW6g1dCCF6ggZ0IYToCRrQhRCiJ8yd\n2+Jq08WVsDW0vKpU3qqnV7p2pVGzaxjrj1Fj5P2ykHZgPJyctc2ogVbh5EzmFhfdI1mnrFzQeFvU\neXm9cjXrko6g0k1ZD2fdPGrjrPPyNtaGAWDHjss13hcWFtLrXrx4cbR88uTJsW1PPvnkaJnd9qJu\nzNvY1idO5MXsq+eHn7vqewLr2mwL/jswbht+NnkZGP9d8P0Axu3O/Y1tYtuwLaJLKD9bTzzxBFrQ\nG7oQQvQEDehCCNETZiq5bNiwAdu2bVtyG08vKtetLPl8JXWwDBD34+ladGXiaR5vi1Mj3sbuTzG7\nHcNtj7IAuzUdO3ZstBwzxLF80FqPkPsf5RKeXt56661j23idp6uLi4tj+2VJ+mPxB57GV1TTbiaT\nn6rnoormY1tHu3MGvqwwAjB+fw4cODBafvzxx9M2VfIT/3aqe7Bnz54lj4nZG9nFtDUCMkZOsnzA\nUY/PPffc2H4//vGPR8t8T7LxAGgv3JFlVATG+3X06NHRcpRceJ2lk2hbbm+UY1jSybJGAvWYw3C/\nJLkIIcScoQFdCCF6wkwllxdffDGdbmYeFlG2yKK94vSHpzmZR0V1vrieJQOKba88RVgWqab77LER\np65MJp9EW3AfWRKJ8givx6kwTyczKQrIaz3GKDieGleRoq3FH1g+4P0qrxm2S+XxE/vIx1V95MjJ\nrL+RymOD7wFHQMb7ndXijDIX3yte5nbH9crjh59VlliAcTtVEbWtyblak8Vlx8TnJ6vrWrW1erb4\neYzFOTKZKV6LbfblL385bQejN3QhhOgJyw7oZvZJMztnZk/Q364zs4fN7Mjw3y3VOYQQQqw+LW/o\nfw/gnvC3+wHsd/dFAPuH60IIIdaQZTV0d/83M9sV/nwvgLuGyw8A+AqADy13roWFhZHWG/Ui1qNY\nc4rRWKwdssYY92NdsdKXK7e4rGBxVeS2ItOKo6aaZQGMGi27NVURbOzGxu5Z0XWL7VllM+T2Hjx4\ncGw/1luzKMK4H+uw0f0ycwmNuiQ/P6xxc9+BcTc0Pia6lrENY5tY22RdOt5H7he3t/rukPUXGC88\nwTpsvN/cF44AjVkjub3sqsjuh8D4fYwunHwfWTevnmmma4ZK7kuWQROov3EwXVyA47WyTKPxW0jm\nthi/780y2+JWdz8zXD4LYGnnYyGEEDNjYi8Xd3cz82y7me0FsBcA1q3TN1ghhFgtug7oT5nZje5+\nxsxuBHAu29Hd9wHYBwCbNm3yS9P8ONXK3AfjdJL3Y/mlSsrPy1XhhtimLtJKVRiBp+A8VY3TWF7n\nqMToCsbw9DTKSpnMEiUXtmF0XePo1SoJE6/zMbEYAE/JqyRjre6sXHSDn4voZsdTa+5/JdlVicq4\nvVGyy6bdcQrO/edzRxkokxij/JRFKFdRlFWkKD+D8VnlQhaVRJAVIIm/udbkdtnvtrXebVVYg88X\n72lVyzWTUqp6qEzlmtlK11fmhwDcN1y+D8AXOp5HCCHElGhxW/w0gK8BeL2ZnTKz9wL4KIC3m9kR\nAG8brgshhFhDWrxc3p1sunulF7tw4cLoC3rXKEqernAu5Tg14vXWSNHKo6ZKupVN0eJ+3EeeTsWv\n6ryNZZ9YyzOLety5c+fYfuwREfvP8JQ8SiRZgqbWeolRwmE7VdG6WdK2CG9jiSDKG9wm9gCJU3W+\nJ611bCsPiypSlH8LVYRqFq1cyRTshROTh/E9aa0HWuV/Z6pEalXEL5PVBYhU/c/kmOoY7mP8bXI0\nbNyWPTOtSeWmgb5SCiFET9CALoQQPUEDuhBC9ISZZlt095FmVGlHrGFF7TXTvipdmzXaWH+RawlG\nfZD1Rz5HzFrHGlmr22L1naBVp2RYY4znY807RgEyfE8qDb1yucyiYaMOXbl/tbRvGvD3iajlZhkL\nKyoXt+pbDdOlJm0F2yw+t7xeFerIvv1EWt0R2dbRNZh/P5WGzufg5VatvdXOUSdvtVOX81fjTyt6\nQxdCiJ6gAV0IIXrCTCUXTs7VSpxCZdOwKG9k06voMlZJKQxPXePUKEsAFK+VRdzF/XgaVkk4vM4y\nSGxfTIx1iWhbvm5sUyaltEazVa5blfyUFTKo5A2W36KbJktsvMwuqvG4yk5VYijer0qyxm1vjU6u\n5A12x+xC7EerzNJK9fxk22Kbst93FW3ZKoNU163caDO5aJboDV0IIXqCBnQhhOgJGtCFEKInzFRD\nv3jxYplkvoXMdS2GSWeZ+aKmWhXJ4JD5eP4WostTVsi4SlvAx0TNLtNyo37H+nqrfhnh87NuWhXb\nrcjaEduQnS9el9M78D2NGSV5nXXz+FxwH+Mzm6U0iC62XBjiwIEDo+XHHntsbD9uxxvf+MamNrVm\nCWVdd1JtfSmm7Uqa6dfxOtU3mZVeJ65nv6uqfUC7myHfB76PlQtnK3pDF0KInqABXQghesJMJRcz\n6+TmNClZjU5g3E0uRlGyVMNue5X8wlOyOMXlKVR1XW5vVZyDr1W5N/L5qykj05rpr3JDZVtHd8TW\nqXqW5TJmUbz55ptHyyyr3HrrrWP7VUVRGJ52RymFC0AcPXp0tBwzT7K7KPc/FipptQXvV9W9zKSV\nOKXPMjvGAimVTNpa2zOT7KqIUpacqhq3vC1KKZwdsdWFkZ/pWDyEI82jnPfa1752tMw2rMY8lmni\nc6ZIUSGEmGM0oAshRE9Ys+RckbWQYiJxqsrr1TSR4f7FKS6vZ1+6gfGpIU93YwRo5h1S1Tps9WSp\n6mOyvBOn47yNpYUYecpUHj88xW2d7vK2KB+wLarIWPZkiZG2Wd3UmKiMZRa2SzXdZ3tyZDFQF1lh\nWp/VLsmqoucFP5Ot0ZFVm/gc0Z4ttHqlVJHG1W+pqmvL62s1nukNXQgheoIGdCGE6Aka0IUQoidc\nMRr6rIiaGGvZlebNx1URd1Vx2CwCNMIaXpbJEcjd3aK2x9prpZVW26KrXcsx1X7sksY6N0fnAsCe\nPXtGy+yCWEWA8rmrIr9VseuDBw+OlmOxD17n80V3Ub6PWUGGCGv50dWTXeh4ObqO8jNYuZVmRTyi\nFt5a8Jj7H8+R6deVRs1Uv5cqy2Gm60eXVW7T4uLiaDkWXM+eM2DcTvzcxWeQ3RP5GWQX2KWOa0Fv\n6EII0RM0oAshRE+YqeTys0aWAKma/nF0VyUvVVJFlWgro0qYlbm4RVe9TOoBxqd/PD2N5968efOS\n2+JUfffu3aNldjl83eteN7YfF6HYsWPHaDlKCXx+bmt0OeT1Q4cOLbkMjEswLKsAwLPPPjtaziIg\nK+K9z5KnVVNuPke0LcsJlUsoSwaVDMK2jnbnfTkiOdqMJbuqj9Vz3EJ8HjM7xUjjLKEbyy9xW5Rt\nsoj0+AyePHlytHz48OHRMst8wE9LeC0s+4ZuZjvM7BEzO2hm3zGz9w//fp2ZPWxmR4b/blnx1YUQ\nQkyNFsnlAoAPuvseAHcCeJ+Z7QFwP4D97r4IYP9wXQghxBqxrOTi7mcAnBkuv2BmhwBsA3AvgLuG\nuz0A4CsAPrQqrWwgThNZFqki5zJPltWmtU5jNbXOpviV5wBLKXG625oHmq8bvVI4pzxHdsZc8yyf\nsOQSa3sy3KY4jeWoQp76R++VLlGeVWKoKmIzSxoV7Z7Vxo37ZeerPEqyRG/AuKcMywfRg4j7GPvL\ntuDzHTt2bGy/rI/xOcsiimMfMykp9pGllNY8+bzMz3A8R4R/W/w8scQCAN/73vdGy/wcR2+qKHu2\nsKKPoma2C8CbAXwdwNbhYA8AZwHkPRVCCLHqNL+SmtlmAJ8D8AF3f97MRtvc3c3Mk+P2AtgLAOvW\nyalGCCFWi6YR1sw2YDCYf8rdPz/881NmduNw+40Azi11rLvvc/fb3f12DehCCLF6LPuGboNX8U8A\nOOTuH6NNDwG4D8BHh/9+oeFcqe57JWRbXG2yAgVRN+T9ukZ2MpneGm3O141RcOxCxvo3F5YAcv0x\nao+ZBhp1Y9YiWWOs3BFZv43aOOvrlcsh2za6p7W6BfI5q+IP2TGxTVmdUy64EeH92AUUGP9ewfc3\n6tC8Hp8L/obC2SFj0Ra+D9ym6tmvyFxiKw2dI5LjNx3eL4ugje2NkbzsfsrPavyOk327qdyIW2kZ\nEd4C4HcBPG5m3x7+7cMYDOQPmtl7AZwA8M4VX10IIcTUaPFy+SoASzbfPd3mCCGE6MpMI0XXrVtX\nJveZhFm6HMYk/0wV5ZlFYkaZoXXa2WlKltRsBManodEdMSs0ESUXPkcVKZpF0kWJhKenLC1EFy+W\nWeJUOIOn4BzhGrdFO2WJsWIxjUwKiK5/PNXmCMvYj0yqqGqZVi6mmSQWXfq4/1u2jMcP8nEcVVlF\nGlcJ57i9LG21/r4r6axV1mXbRmmPpaTojnjixInRMj/H8T5mdU67/J4j+kophBA9QQO6EEL0BA3o\nQgjRE66YbItdXPBaz9f13FmB2dYi0VELZ+29VWuvNLYumhvrt1Eb5vWYZY7XWYuNuizryJluCuQ6\nZQwZZ82SNfSovbItuI9RG+dtlU7O65XbImvI8RxZ2Hl0meP+P/roo6Pl2MfMZpWLIN+DeK94ne/v\nLbfcMrYfZ8CMro/swlq5cGZpC6rvURWc1bQqVJ0VXI8ugrzO32fi81iF6vN6a6ZMfmaqghmx+EWG\n3tCFEKInaEAXQoieMFPJ5RWveMVoahflCHbt4eU4VY/T0EvESESeWvMUOU4Z2e0uZlbjffl8cVrH\n08asriAwntmRp1ex7ZkcU2VH5GlclUmOp9ZV8v6qHioTbVFFLTJspyxKDwBuu+22JY+PdV2zrJlR\nLuGISD6mkgiim2FWU5ZlgIpqqs7ySeXSV9mMZaaqzidfl39zsdhHFSnKslW1X+bCWsmXWaQtkNfs\njFIK94u3VZJLdb5WN8PWPvI5Wt1tK/SGLoQQPUEDuhBC9ISZSi7nz5//qa/Gl+CpIcsscbqS1UGM\nX/B5SsrbYgEFnpLHxD48TaySJnE7qnqjLOFUX8FbE1dlkkuUjjKvlGizyrOD21F5JrTKDjxVj+2Y\nJtV0N5NOgPo+cv+rGrKZdFbJjSxZxSjFLLo4ygIs1VTPWZYsLh7D7YuRvPwbqbzJsqItkcwzqqqv\nWUkfrVHXrfsxVfK01qR6045w1xu6EEL0BA3oQgjREzSgCyFETzD3JSvHrQrr1q1zdhtjsmx0MdKP\nNXDWfKtMchyZx5FtcVurhl7pXpVWmrkgRtfMLKNb1BGzgroxEpH16izhPzBuz6owchXlmmmRXTPJ\nterVWRuq9lURi9W1smLa0c0wixqOGnWmm0f3xixbZ2XbVn25+tbQJdK69XtCpfFXZG1q7Uelf7dc\nZyVMI4vi8ePHH3X325fbT2/oQgjREzSgCyFET5ip2+LCwsJI1ohTnCyRUZRSdu7cOVpm+aSqEcgu\neFHyyWSViq7TxKr2IcPbeLlK1sRtqmpbZpG28XyRbMpc1YSsptYscbCUVMlUlZTSKs1kUkWrNBPP\n0Sq5tNqC6SqlMJWLHP8WKmmh1YWT29Qq2VXt7VLgojpfF1ZyDyrJiZm2LMnoDV0IIXqCBnQhhOgJ\nM5Vc1q9fP5JG4jSOpRWWS2LUI3u5sMxS1XNk4rS4iqrjqXUVAdkaZdaaUIi3cf3B1rZ3rb/ItHp2\nRJtl087KRq2eGK15tKchVVS09rFVLuL71ZrUaRqRjZPu15XVljanSVdbtCYda/2ttibu0hu6EEL0\nBA3oQgjREzSgCyFET5iphn7hwoVRFGTUmFgTrbK7cVRdFl0az1/pnLxeRWK2aqUVrZF+fP7Weold\nMs5NQyudRja71nZ00eFbMwBW2+I5MlfArv3IrtUakdw122C2bRr3qqLKXJr1udUWK9nGdLFFxTQi\nb7uw7Bu6mW00s/8ws8fM7Dtm9pHh368zs4fN7Mjw3y2r1kohhBDL0iK5/C+At7r7bQDeBOAeM7sT\nwP0A9rv7IoD9w3UhhBBrxIqSc5nZNQC+CuAPAPwDgLvc/YyZ3QjgK+7++ur4hYUF37Rp05LbMnet\nOHXJotu6RLPF9SoicBpJ9Lkvre5pFZPKHa2SA9BtKtxlmjyNRFNMq21X0o/VjD6cZTKoLvegYhpF\nHbLasF1dOLO+RLfXadgi27erbMOcPn16esm5zGzBzL4N4ByAh9396wC2uvuZ4S5nAWxNTyCEEGLV\naRrQ3f2iu78JwHYAd5jZG8J2B7Dkq76Z7TWzb5rZN2eZqlcIIeaNFbktuvuPADwC4B4ATw2lFgz/\nPZccs8/db3f3281s0vYKIYRIWFawM7NXA3jZ3X9kZlcDeDuAvwDwEID7AHx0+O8XljvXxo0bsbi4\nuGyjWsO6W123KiptL9P6phE+vppFZKvvBC1/X+q6k2qg1bW7hvG3MA39MjKp3lq5zlbnm0ZKh1kc\nPwlZBtGYBmBSW7R+I1oJq+mO2NyGhn1uBPCAmS1g8Eb/oLt/0cy+BuBBM3svgBMA3rmK7RRCCLEM\nyw7o7n4AwJuX+PvTAO5ejUYJIYRYOTOtKWpmP8Dgbf7nAfxwZhe+spEtLiNbXEa2GCA7DNjp7q9e\nbqeZDuijiw48Xpb1qZwHZIvLyBaXkS0GyA4rQ8m5hBCiJ2hAF0KInrBWA/q+NbrulYhscRnZ4jKy\nxQDZYQWsiYYuhBBi+khyEUKInjDTAd3M7jGz75rZUTObq3S7ZrbDzB4xs4PDvPLvH/59bvPKD5O+\nfcvMvjhcn0tbmNnPmdlnzeywmR0ys1+eY1v88fD38YSZfXpYj2EubdGFmQ3ow0jTvwHw6wD2AHi3\nme2Z1fWvAC4A+KC77wFwJ4D3Dfs/z3nl3w/gEK3Pqy3+GsA/ufstAG7DwCZzZwsz2wbgjwDc7u5v\nALAA4F2YQ1t0ZZZv6HcAOOru33f38wA+A+DeGV5/TXH3M+7+n8PlFzD40W7DwAYPDHd7AMBvr00L\nZ4uZbQfwmwA+Tn+eO1uY2bUAfg3AJwDA3c8Pk+DNnS2GrAdwtZmtB3ANgCcxv7ZYMbMc0LcBOEnr\np4Z/mzvMbBcG6RTmOa/8XwH4UwD/R3+bR1vsBvADAH83lJ8+bmabMIe2cPfTAP4SwH8DOAPgOXf/\nF8yhLbqij6Izxsw2A/gcgA+4+/O8rcor3yfM7B0Azrn7o9k+82ILDN5IfxHA37r7mwH8D4KkMC+2\nGGrj92Lwn9xNADaZ2Xt4n3mxRVdmOaCfBrCD1rcP/zY3mNkGDAbzT7n754d/bsor3zPeAuC3zOw4\nBtLbW83sHzGftjgF4NSwChgAfBaDAX4ebfE2AMfc/Qfu/jKAzwP4FcynLToxywH9GwAWzWy3mV2F\nwceOh2Z4/TXFBtU9PgHgkLt/jDZdyisPNOaV/1nH3f/M3be7+y4MnoN/dff3YD5tcRbASTO7VI/3\nbgAHMYe2wEBqudPMrhn+Xu7G4FvTPNqiE7POtvgbGGinCwA+6e5/PrOLrzFm9qsA/h3A47isG38Y\nAx39QQC/gGFeeXd/Zk0auQaY2V0A/sTd32Fm12MObWFmb8Lg4/BVAL4P4PcwrD2A+bPFRwD8DgZe\nYd8C8PsANmMObdEFRYoKIURP0EdRIYToCRrQhRCiJ2hAF0KInqABXQgheoIGdCGE6Aka0IUQoido\nQBdCiJ6gAV0IIXrC/wNSP9KQcXhoQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119c7c588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0][:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv-net model\n",
    "\n",
    "The base CNN has five convolutional layers and two fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as ks\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras.layers import Flatten, Dropout, MaxPooling2D,Input\n",
    "from keras.layers import Dense, InputLayer, Convolution2D\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def squeeze_dim(x, axis=-1):\n",
    "    # Removes a 1-dimension from the tensor at index \"axis\"\n",
    "    return K.squeeze(x, axis=axis)\n",
    "\n",
    "\n",
    "class CNN(object):\n",
    "    \"\"\"\n",
    "    Usage for tf tensor output:\n",
    "    o = CNN(x).tf_output()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_tensor):\n",
    "        self._build_network(input_tensor)\n",
    "\n",
    "    def _build_network(self, input_tensor):\n",
    "        \n",
    "        model_input = Input(shape=(32,100,1))#,tensor=input_tensor)\n",
    "\n",
    "        conv_layers_size = [64,128,256,512,512]\n",
    "        edges_size = [5,5,3,3,3]\n",
    "        \n",
    "        #convolutional layes\n",
    "        x = model_input \n",
    "        for i,(layer_size, edge_size) in enumerate(zip(conv_layers_size,edges_size)):\n",
    "            x = Convolution2D(\n",
    "                layer_size, edge_size, edge_size, \n",
    "                activation='relu', subsample=(1, 1),border_mode='same')(x)\n",
    "            if i <= 2 :\n",
    "                x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "        #dense layers\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(4096, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(4096)(x)\n",
    "    \n",
    "    \n",
    "        N = 23\n",
    "        outputs = []\n",
    "        for i in range(N):\n",
    "            outputs.append(Dense(37,activation='softmax')(x))\n",
    "        \n",
    "        self.model = Model(input=model_input,output=outputs)\n",
    "\n",
    "    def tf_output(self):\n",
    "        # if self.input_tensor is not None:\n",
    "        return self.model.output\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        return self.model(input_tensor)\n",
    "    \n",
    "    def tf_summary(self):\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save_weights(str(filename) + \".h5\")\n",
    "        print(\"Model saved to disk\")\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "        print(\"Load model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_cnn = CNN(tf.Variable(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 32, 100, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (None, 32, 100, 64)   1664        input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_25 (MaxPooling2D)   (None, 16, 50, 64)    0           convolution2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_44 (Convolution2D) (None, 16, 50, 128)   204928      maxpooling2d_25[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_26 (MaxPooling2D)   (None, 8, 25, 128)    0           convolution2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_45 (Convolution2D) (None, 8, 25, 256)    295168      maxpooling2d_26[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_27 (MaxPooling2D)   (None, 4, 12, 256)    0           convolution2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_46 (Convolution2D) (None, 4, 12, 512)    1180160     maxpooling2d_27[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_47 (Convolution2D) (None, 4, 12, 512)    2359808     convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 24576)         0           convolution2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_179 (Dense)                (None, 4096)          100667392   flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 4096)          0           dense_179[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_180 (Dense)                (None, 4096)          16781312    dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_181 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_182 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_183 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_184 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_185 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_186 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_187 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_188 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_189 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_190 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_191 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_192 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_193 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_194 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_195 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_196 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_197 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_198 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_199 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_200 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_201 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_202 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_203 (Dense)                (None, 37)            151589      dense_180[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 124,976,979\n",
      "Trainable params: 124,976,979\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_cnn.tf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk\n"
     ]
    }
   ],
   "source": [
    "model_cnn.save('model_null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = predict_batch(\"try\", (32, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = \"0123456789abcdefghijklmnopqrstuvwxyz \"\n",
    "maxLen = 23\n",
    "def word_to_labels(w):\n",
    "    w_to_int = []\n",
    "    letters = list(w) + [\" \"]*(maxLen - len(w))\n",
    "    for letter in letters :\n",
    "        w_to_int.append(alphabet.index(letter))\n",
    "    return np.array(w_to_int)\n",
    "\n",
    "def labels_to_word(labels):\n",
    "    w = \"\"\n",
    "    for label in labels:\n",
    "        w+=alphabet[label]\n",
    "    \n",
    "    return w.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey salut'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_word(word_to_labels(\"hey salut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = []\n",
    "for word in labels:\n",
    "    Y.append(word_to_labels(word))\n",
    "Y = np.array(Y)\n",
    "Y = Y.T #la première ligne contient toutes les premières letters, la second toutes les secondes lettres ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_labels = []\n",
    "for l in Y : \n",
    "    output_labels.append(to_categorical(l,37))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n",
      "(10000, 37)\n"
     ]
    }
   ],
   "source": [
    "for o in output_labels:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_labels[0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 10000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_word(Y.T[0,100:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = \"0123456789abcdefghijklmnopqrstuvwxyz \"\n",
    "V = len(list(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_sequences):\n",
    "    rev_vocabulary = []\n",
    "    unique_tokens = set()\n",
    "    for tokens in tokenized_sequences:\n",
    "        unique_tokens.update(tokens)\n",
    "    rev_vocabulary += sorted(unique_tokens)\n",
    "    vocabulary = {}\n",
    "    for i, token in enumerate(rev_vocabulary):\n",
    "        vocabulary[token] = i\n",
    "    return vocabulary, rev_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary, rev_vocabulary = build_vocabulary(list(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_labels(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(w):\n",
    "    letters = word_to_labels(w)\n",
    "    vector = np.zeros((maxLen,V))\n",
    "    for i, letter in enumerate(letters) :\n",
    "        vector[i,vocabulary[letter]]=1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 37)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize(\"abc\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 37)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize('abcade').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = []\n",
    "labels = []\n",
    "for word in y :\n",
    "    #v = vectorize(word)\n",
    "    Y.append(vectorize(word))\n",
    "    \n",
    "# Y.append(vectorize(word))\n",
    "Y = np.array(Y)\n",
    "Y = np.rollaxis(Y,1)\n",
    "#Y = Y.T\n",
    "Y_ = []\n",
    "for row in Y:\n",
    "    Y_.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = vectorize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 523, 37)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-5b744c775286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_' is not defined"
     ]
    }
   ],
   "source": [
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split \n",
    "#X, y = predict_batch(\"try\", (32, 100))\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#...     X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470 samples, validate on 53 samples\n",
      "Epoch 1/15\n",
      "81s - loss: 120.9547 - dense_181_loss: 13.4220 - dense_182_loss: 12.1586 - dense_183_loss: 13.3131 - dense_184_loss: 13.4676 - dense_185_loss: 13.2484 - dense_186_loss: 12.4473 - dense_187_loss: 13.7342 - dense_188_loss: 8.2421 - dense_189_loss: 6.0816 - dense_190_loss: 4.2640 - dense_191_loss: 2.5836 - dense_192_loss: 1.4176 - dense_193_loss: 1.0747 - dense_194_loss: 0.8689 - dense_195_loss: 0.6289 - dense_196_loss: 0.5260 - dense_197_loss: 0.5260 - dense_198_loss: 0.4917 - dense_199_loss: 0.4917 - dense_200_loss: 0.4917 - dense_201_loss: 0.4917 - dense_202_loss: 0.4917 - dense_203_loss: 0.4917 - dense_181_acc: 0.0596 - dense_182_acc: 0.1362 - dense_183_acc: 0.0660 - dense_184_acc: 0.0681 - dense_185_acc: 0.0723 - dense_186_acc: 0.1255 - dense_187_acc: 0.0489 - dense_188_acc: 0.3830 - dense_189_acc: 0.5191 - dense_190_acc: 0.6298 - dense_191_acc: 0.7362 - dense_192_acc: 0.8149 - dense_193_acc: 0.8298 - dense_194_acc: 0.8596 - dense_195_acc: 0.8638 - dense_196_acc: 0.8660 - dense_197_acc: 0.8617 - dense_198_acc: 0.8638 - dense_199_acc: 0.8638 - dense_200_acc: 0.8638 - dense_201_acc: 0.8638 - dense_202_acc: 0.8660 - dense_203_acc: 0.8638 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 2/15\n",
      "65s - loss: 128.0532 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.0207 - dense_185_loss: 14.8492 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0681 - dense_185_acc: 0.0787 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 3/15\n",
      "64s - loss: 128.0874 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.1579 - dense_185_loss: 14.7463 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0596 - dense_185_acc: 0.0851 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 4/15\n",
      "107s - loss: 127.9846 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.0207 - dense_185_loss: 14.7463 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1565 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0681 - dense_185_acc: 0.0851 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4319 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 5/15\n",
      "108s - loss: 127.9503 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.0550 - dense_185_loss: 14.7121 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0660 - dense_185_acc: 0.0872 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 6/15\n",
      "108s - loss: 128.0532 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.1922 - dense_185_loss: 14.6778 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0574 - dense_185_acc: 0.0894 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 7/15\n",
      "112s - loss: 127.8817 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 14.9864 - dense_185_loss: 14.7121 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0702 - dense_185_acc: 0.0872 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 8/15\n",
      "77s - loss: 128.0532 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.0893 - dense_185_loss: 14.7806 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0638 - dense_185_acc: 0.0830 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 9/15\n",
      "104s - loss: 127.8131 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 14.9521 - dense_185_loss: 14.6778 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0723 - dense_185_acc: 0.0894 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 10/15\n",
      "127s - loss: 127.9160 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 14.9521 - dense_185_loss: 14.7806 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0723 - dense_185_acc: 0.0830 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 11/15\n",
      "129s - loss: 128.1217 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.2265 - dense_185_loss: 14.7121 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0553 - dense_185_acc: 0.0872 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 12/15\n",
      "132s - loss: 128.1903 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.3293 - dense_185_loss: 14.6778 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0489 - dense_185_acc: 0.0894 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 13/15\n",
      "105s - loss: 127.9846 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.0893 - dense_185_loss: 14.7121 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0638 - dense_185_acc: 0.0872 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 14/15\n",
      "115s - loss: 128.0531 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 15.1579 - dense_185_loss: 14.7121 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0596 - dense_185_acc: 0.0872 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n",
      "Epoch 15/15\n",
      "119s - loss: 127.8474 - dense_181_loss: 15.0207 - dense_182_loss: 13.9919 - dense_183_loss: 14.8835 - dense_184_loss: 14.9521 - dense_185_loss: 14.7121 - dense_186_loss: 13.9576 - dense_187_loss: 15.5351 - dense_188_loss: 9.1222 - dense_189_loss: 6.4815 - dense_190_loss: 4.3210 - dense_191_loss: 2.4692 - dense_192_loss: 1.0631 - dense_193_loss: 0.6516 - dense_194_loss: 0.4115 - dense_195_loss: 0.1715 - dense_196_loss: 0.0686 - dense_197_loss: 0.0343 - dense_198_loss: 1.1921e-07 - dense_199_loss: 1.1921e-07 - dense_200_loss: 1.1921e-07 - dense_201_loss: 1.1921e-07 - dense_202_loss: 1.1921e-07 - dense_203_loss: 1.1921e-07 - dense_181_acc: 0.0681 - dense_182_acc: 0.1319 - dense_183_acc: 0.0766 - dense_184_acc: 0.0723 - dense_185_acc: 0.0872 - dense_186_acc: 0.1340 - dense_187_acc: 0.0362 - dense_188_acc: 0.4340 - dense_189_acc: 0.5979 - dense_190_acc: 0.7319 - dense_191_acc: 0.8468 - dense_192_acc: 0.9340 - dense_193_acc: 0.9596 - dense_194_acc: 0.9745 - dense_195_acc: 0.9894 - dense_196_acc: 0.9957 - dense_197_acc: 0.9979 - dense_198_acc: 1.0000 - dense_199_acc: 1.0000 - dense_200_acc: 1.0000 - dense_201_acc: 1.0000 - dense_202_acc: 1.0000 - dense_203_acc: 1.0000 - val_loss: 127.7283 - val_dense_181_loss: 15.2058 - val_dense_182_loss: 13.9893 - val_dense_183_loss: 14.9016 - val_dense_184_loss: 14.9016 - val_dense_185_loss: 14.2934 - val_dense_186_loss: 14.9016 - val_dense_187_loss: 15.2058 - val_dense_188_loss: 8.8193 - val_dense_189_loss: 6.6905 - val_dense_190_loss: 2.7370 - val_dense_191_loss: 2.4329 - val_dense_192_loss: 1.8247 - val_dense_193_loss: 1.5206 - val_dense_194_loss: 0.3041 - val_dense_195_loss: 1.1921e-07 - val_dense_196_loss: 1.1921e-07 - val_dense_197_loss: 1.1921e-07 - val_dense_198_loss: 1.1921e-07 - val_dense_199_loss: 1.1921e-07 - val_dense_200_loss: 1.1921e-07 - val_dense_201_loss: 1.1921e-07 - val_dense_202_loss: 1.1921e-07 - val_dense_203_loss: 1.1921e-07 - val_dense_181_acc: 0.0566 - val_dense_182_acc: 0.1321 - val_dense_183_acc: 0.0755 - val_dense_184_acc: 0.0755 - val_dense_185_acc: 0.1132 - val_dense_186_acc: 0.0755 - val_dense_187_acc: 0.0566 - val_dense_188_acc: 0.4528 - val_dense_189_acc: 0.5849 - val_dense_190_acc: 0.8302 - val_dense_191_acc: 0.8491 - val_dense_192_acc: 0.8868 - val_dense_193_acc: 0.9057 - val_dense_194_acc: 0.9811 - val_dense_195_acc: 1.0000 - val_dense_196_acc: 1.0000 - val_dense_197_acc: 1.0000 - val_dense_198_acc: 1.0000 - val_dense_199_acc: 1.0000 - val_dense_200_acc: 1.0000 - val_dense_201_acc: 1.0000 - val_dense_202_acc: 1.0000 - val_dense_203_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x37daf0cc0>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_cnn.model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X, Y_,\n",
    "          batch_size=64, nb_epoch=15, \n",
    "          validation_split=0.1,shuffle=True, verbose=2)\n",
    "#model.metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from disk\n"
     ]
    }
   ],
   "source": [
    "model_cnn.load(\"model1.h5\")\n",
    "model = model_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"data.npy\")\n",
    "X = data[()][\"X\"]\n",
    "y = data[()][\"y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 37)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-4145239ac453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrev_vocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "for i in range(23):\n",
    "    s += rev_vocabulary[np.where(y_pred[i][0]==1)[0][0]]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.02702687,  0.02702701,  0.02702659,  0.02702659,  0.0270269 ,\n",
       "          0.02702591,  0.02702723,  0.02702699,  0.02702776,  0.02702725,\n",
       "          0.02702658,  0.02702628,  0.02702715,  0.02702746,  0.02702707,\n",
       "          0.02702693,  0.02702618,  0.02702711,  0.02702759,  0.02702682,\n",
       "          0.02702693,  0.02702673,  0.02702761,  0.02702737,  0.02702584,\n",
       "          0.02702751,  0.02702827,  0.02702721,  0.02702628,  0.02702751,\n",
       "          0.02702658,  0.02702765,  0.02702722,  0.0270271 ,  0.02702734,\n",
       "          0.02702719,  0.02702748]], dtype=float32),\n",
       " array([[ 0.02702802,  0.02702746,  0.02702624,  0.02702689,  0.0270267 ,\n",
       "          0.0270269 ,  0.02702766,  0.02702591,  0.02702694,  0.0270275 ,\n",
       "          0.02702708,  0.02702754,  0.02702763,  0.02702709,  0.02702711,\n",
       "          0.02702727,  0.02702576,  0.02702813,  0.02702658,  0.02702837,\n",
       "          0.02702754,  0.02702713,  0.02702635,  0.0270268 ,  0.02702718,\n",
       "          0.02702616,  0.0270264 ,  0.02702661,  0.02702618,  0.02702608,\n",
       "          0.0270276 ,  0.0270269 ,  0.02702722,  0.02702822,  0.02702826,\n",
       "          0.02702571,  0.02702691]], dtype=float32),\n",
       " array([[ 0.02702676,  0.02702789,  0.02702747,  0.02702842,  0.02702687,\n",
       "          0.02702712,  0.02702688,  0.02702632,  0.02702646,  0.02702716,\n",
       "          0.027027  ,  0.02702726,  0.02702731,  0.02702736,  0.02702739,\n",
       "          0.02702748,  0.02702735,  0.02702715,  0.02702752,  0.02702686,\n",
       "          0.0270266 ,  0.02702768,  0.02702669,  0.02702702,  0.02702658,\n",
       "          0.02702597,  0.02702669,  0.02702811,  0.0270258 ,  0.02702644,\n",
       "          0.02702696,  0.02702638,  0.02702727,  0.02702688,  0.02702618,\n",
       "          0.02702766,  0.02702693]], dtype=float32),\n",
       " array([[ 0.02702635,  0.02702717,  0.02702692,  0.0270266 ,  0.02702795,\n",
       "          0.02702749,  0.02702725,  0.02702695,  0.02702789,  0.02702683,\n",
       "          0.02702727,  0.0270263 ,  0.02702748,  0.02702715,  0.02702716,\n",
       "          0.02702757,  0.02702739,  0.02702715,  0.02702653,  0.02702663,\n",
       "          0.0270256 ,  0.02702778,  0.02702733,  0.02702767,  0.02702803,\n",
       "          0.02702767,  0.02702629,  0.02702627,  0.02702703,  0.02702638,\n",
       "          0.02702738,  0.02702571,  0.02702689,  0.02702762,  0.02702663,\n",
       "          0.02702651,  0.02702729]], dtype=float32),\n",
       " array([[ 0.02702661,  0.02702717,  0.02702731,  0.02702715,  0.02702682,\n",
       "          0.02702664,  0.02702657,  0.02702657,  0.02702751,  0.02702693,\n",
       "          0.02702587,  0.02702649,  0.02702739,  0.0270272 ,  0.02702693,\n",
       "          0.02702776,  0.02702721,  0.02702692,  0.02702794,  0.02702639,\n",
       "          0.02702694,  0.02702838,  0.02702732,  0.02702627,  0.02702697,\n",
       "          0.02702737,  0.02702738,  0.02702683,  0.02702642,  0.02702712,\n",
       "          0.02702583,  0.02702698,  0.02702768,  0.02702727,  0.02702633,\n",
       "          0.02702787,  0.02702767]], dtype=float32),\n",
       " array([[ 0.02702662,  0.02702691,  0.02702759,  0.02702769,  0.02702616,\n",
       "          0.02702785,  0.02702774,  0.0270272 ,  0.02702638,  0.02702625,\n",
       "          0.0270266 ,  0.02702698,  0.0270264 ,  0.0270271 ,  0.02702669,\n",
       "          0.02702668,  0.02702647,  0.02702677,  0.02702735,  0.02702733,\n",
       "          0.02702799,  0.02702731,  0.02702689,  0.02702786,  0.02702742,\n",
       "          0.02702791,  0.02702754,  0.02702764,  0.02702804,  0.02702592,\n",
       "          0.02702705,  0.02702631,  0.0270262 ,  0.02702657,  0.02702576,\n",
       "          0.02702753,  0.02702719]], dtype=float32),\n",
       " array([[ 0.02702612,  0.027027  ,  0.02702684,  0.02702682,  0.02702734,\n",
       "          0.02702698,  0.02702734,  0.02702706,  0.0270287 ,  0.02702703,\n",
       "          0.02702738,  0.02702704,  0.02702608,  0.0270265 ,  0.02702682,\n",
       "          0.02702743,  0.02702742,  0.02702561,  0.02702655,  0.02702615,\n",
       "          0.02702775,  0.02702734,  0.02702725,  0.02702715,  0.02702738,\n",
       "          0.02702694,  0.02702809,  0.02702725,  0.02702765,  0.0270278 ,\n",
       "          0.02702664,  0.02702705,  0.02702646,  0.02702774,  0.02702616,\n",
       "          0.02702683,  0.02702633]], dtype=float32),\n",
       " array([[ 0.02702695,  0.02702666,  0.02702624,  0.02702782,  0.02702721,\n",
       "          0.02702728,  0.02702632,  0.02702689,  0.0270261 ,  0.02702734,\n",
       "          0.02702847,  0.02702783,  0.02702656,  0.02702601,  0.02702777,\n",
       "          0.02702671,  0.02702661,  0.0270263 ,  0.02702736,  0.02702748,\n",
       "          0.02702626,  0.02702685,  0.02702714,  0.02702645,  0.02702865,\n",
       "          0.0270265 ,  0.02702769,  0.02702805,  0.02702605,  0.02702708,\n",
       "          0.02702597,  0.02702756,  0.0270274 ,  0.02702761,  0.02702703,\n",
       "          0.02702671,  0.02702711]], dtype=float32),\n",
       " array([[ 0.02702726,  0.02702683,  0.02702642,  0.02702674,  0.02702771,\n",
       "          0.0270268 ,  0.02702771,  0.02702595,  0.02702829,  0.02702757,\n",
       "          0.02702699,  0.02702674,  0.02702808,  0.02702692,  0.02702808,\n",
       "          0.02702592,  0.02702743,  0.02702738,  0.02702602,  0.02702649,\n",
       "          0.0270268 ,  0.02702666,  0.0270275 ,  0.02702746,  0.0270266 ,\n",
       "          0.02702628,  0.0270274 ,  0.02702678,  0.02702756,  0.02702615,\n",
       "          0.02702707,  0.02702698,  0.0270273 ,  0.02702703,  0.02702667,\n",
       "          0.02702751,  0.02702696]], dtype=float32),\n",
       " array([[ 0.02702635,  0.02702608,  0.02702734,  0.02702766,  0.02702712,\n",
       "          0.02702773,  0.0270279 ,  0.02702774,  0.02702611,  0.02702704,\n",
       "          0.02702629,  0.02702834,  0.02702685,  0.02702748,  0.0270263 ,\n",
       "          0.02702716,  0.02702578,  0.02702729,  0.02702674,  0.02702753,\n",
       "          0.02702577,  0.02702826,  0.02702649,  0.02702674,  0.02702644,\n",
       "          0.02702665,  0.02702655,  0.02702716,  0.02702712,  0.02702635,\n",
       "          0.02702739,  0.02702804,  0.02702719,  0.02702755,  0.02702739,\n",
       "          0.02702678,  0.02702728]], dtype=float32),\n",
       " array([[ 0.0270264 ,  0.02702723,  0.02702717,  0.02702727,  0.02702582,\n",
       "          0.02702753,  0.02702662,  0.0270278 ,  0.02702595,  0.02702733,\n",
       "          0.02702781,  0.02702804,  0.02702618,  0.02702788,  0.02702678,\n",
       "          0.02702613,  0.027027  ,  0.02702649,  0.02702754,  0.02702808,\n",
       "          0.02702734,  0.02702556,  0.02702831,  0.02702671,  0.02702679,\n",
       "          0.0270272 ,  0.02702736,  0.02702773,  0.02702667,  0.02702856,\n",
       "          0.0270266 ,  0.02702649,  0.02702629,  0.02702735,  0.02702685,\n",
       "          0.02702673,  0.02702654]], dtype=float32),\n",
       " array([[ 0.0270276 ,  0.02702656,  0.02702653,  0.02702603,  0.02702661,\n",
       "          0.02702718,  0.0270267 ,  0.02702725,  0.02702719,  0.02702636,\n",
       "          0.02702768,  0.02702793,  0.02702666,  0.02702737,  0.02702751,\n",
       "          0.02702736,  0.02702729,  0.02702685,  0.02702741,  0.02702544,\n",
       "          0.02702772,  0.02702528,  0.0270272 ,  0.02702785,  0.02702789,\n",
       "          0.02702655,  0.02702661,  0.02702605,  0.0270275 ,  0.02702662,\n",
       "          0.02702729,  0.02702779,  0.02702727,  0.02702762,  0.02702769,\n",
       "          0.02702739,  0.02702603]], dtype=float32),\n",
       " array([[ 0.02702743,  0.02702646,  0.02702695,  0.02702726,  0.02702784,\n",
       "          0.02702687,  0.02702742,  0.02702759,  0.02702807,  0.02702633,\n",
       "          0.0270273 ,  0.02702625,  0.02702739,  0.02702712,  0.02702662,\n",
       "          0.02702726,  0.02702686,  0.02702751,  0.02702694,  0.02702734,\n",
       "          0.02702676,  0.02702758,  0.02702568,  0.02702685,  0.02702633,\n",
       "          0.02702801,  0.02702588,  0.02702737,  0.02702666,  0.02702662,\n",
       "          0.02702759,  0.02702674,  0.02702758,  0.02702654,  0.02702663,\n",
       "          0.02702731,  0.02702708]], dtype=float32),\n",
       " array([[ 0.0270261 ,  0.02702716,  0.02702589,  0.02702713,  0.02702823,\n",
       "          0.02702661,  0.02702644,  0.02702811,  0.0270268 ,  0.0270271 ,\n",
       "          0.02702717,  0.02702691,  0.02702683,  0.02702669,  0.02702654,\n",
       "          0.02702735,  0.02702656,  0.02702735,  0.0270268 ,  0.02702704,\n",
       "          0.02702718,  0.02702609,  0.02702729,  0.02702719,  0.02702661,\n",
       "          0.027027  ,  0.02702768,  0.02702724,  0.02702771,  0.02702595,\n",
       "          0.02702753,  0.02702827,  0.02702749,  0.02702633,  0.02702684,\n",
       "          0.02702708,  0.02702774]], dtype=float32),\n",
       " array([[ 0.02702601,  0.02702572,  0.02702721,  0.02702756,  0.02702641,\n",
       "          0.02702726,  0.02702734,  0.0270273 ,  0.02702833,  0.02702626,\n",
       "          0.02702703,  0.02702751,  0.02702633,  0.02702752,  0.02702747,\n",
       "          0.02702648,  0.02702697,  0.02702663,  0.02702723,  0.0270271 ,\n",
       "          0.02702769,  0.02702739,  0.02702603,  0.02702686,  0.02702678,\n",
       "          0.0270271 ,  0.02702664,  0.02702724,  0.02702636,  0.02702672,\n",
       "          0.0270267 ,  0.02702765,  0.02702747,  0.0270276 ,  0.02702756,\n",
       "          0.02702707,  0.02702748]], dtype=float32),\n",
       " array([[ 0.02702759,  0.02702703,  0.02702673,  0.02702694,  0.02702748,\n",
       "          0.02702763,  0.02702683,  0.02702669,  0.0270275 ,  0.02702847,\n",
       "          0.02702693,  0.02702825,  0.02702677,  0.02702696,  0.02702672,\n",
       "          0.02702712,  0.02702841,  0.02702626,  0.0270261 ,  0.02702584,\n",
       "          0.02702765,  0.02702668,  0.02702586,  0.02702648,  0.02702816,\n",
       "          0.02702584,  0.02702717,  0.02702739,  0.02702786,  0.02702669,\n",
       "          0.02702731,  0.02702657,  0.02702661,  0.02702685,  0.0270274 ,\n",
       "          0.02702644,  0.0270269 ]], dtype=float32),\n",
       " array([[ 0.02702733,  0.02702644,  0.02702814,  0.02702653,  0.02702728,\n",
       "          0.02702738,  0.02702727,  0.02702705,  0.027027  ,  0.02702767,\n",
       "          0.02702685,  0.0270275 ,  0.02702749,  0.0270266 ,  0.02702674,\n",
       "          0.02702694,  0.02702837,  0.02702582,  0.02702806,  0.02702651,\n",
       "          0.02702656,  0.02702652,  0.02702602,  0.02702653,  0.02702708,\n",
       "          0.0270266 ,  0.02702729,  0.02702617,  0.02702601,  0.02702742,\n",
       "          0.02702731,  0.02702725,  0.02702813,  0.02702618,  0.02702642,\n",
       "          0.02702862,  0.02702698]], dtype=float32),\n",
       " array([[ 0.02702732,  0.02702728,  0.02702723,  0.02702718,  0.02702758,\n",
       "          0.02702834,  0.02702669,  0.02702773,  0.0270277 ,  0.02702756,\n",
       "          0.02702651,  0.02702613,  0.02702826,  0.02702641,  0.02702778,\n",
       "          0.02702745,  0.02702659,  0.02702685,  0.02702635,  0.02702682,\n",
       "          0.02702734,  0.02702559,  0.02702642,  0.02702576,  0.02702643,\n",
       "          0.02702669,  0.02702644,  0.02702691,  0.02702682,  0.02702814,\n",
       "          0.02702783,  0.02702782,  0.02702505,  0.02702743,  0.02702714,\n",
       "          0.02702848,  0.02702609]], dtype=float32),\n",
       " array([[ 0.027027  ,  0.02702686,  0.0270257 ,  0.02702712,  0.0270273 ,\n",
       "          0.02702674,  0.02702755,  0.02702778,  0.02702653,  0.02702789,\n",
       "          0.02702663,  0.02702743,  0.02702809,  0.02702719,  0.02702665,\n",
       "          0.02702709,  0.02702716,  0.02702743,  0.02702737,  0.02702759,\n",
       "          0.02702725,  0.02702652,  0.02702746,  0.02702636,  0.02702737,\n",
       "          0.02702669,  0.0270272 ,  0.02702602,  0.02702736,  0.02702748,\n",
       "          0.02702681,  0.02702651,  0.02702687,  0.02702688,  0.02702788,\n",
       "          0.02702668,  0.02702565]], dtype=float32),\n",
       " array([[ 0.02702662,  0.02702733,  0.02702696,  0.02702728,  0.02702758,\n",
       "          0.02702718,  0.02702695,  0.02702719,  0.0270272 ,  0.02702677,\n",
       "          0.02702827,  0.02702696,  0.02702645,  0.02702806,  0.02702697,\n",
       "          0.0270273 ,  0.02702674,  0.02702647,  0.02702733,  0.02702633,\n",
       "          0.02702792,  0.02702848,  0.02702747,  0.02702725,  0.0270262 ,\n",
       "          0.02702675,  0.02702757,  0.02702669,  0.02702519,  0.02702626,\n",
       "          0.02702796,  0.02702762,  0.02702623,  0.02702769,  0.02702707,\n",
       "          0.02702544,  0.0270263 ]], dtype=float32),\n",
       " array([[ 0.02702735,  0.0270275 ,  0.02702751,  0.02702682,  0.02702661,\n",
       "          0.02702677,  0.0270275 ,  0.02702745,  0.02702656,  0.02702685,\n",
       "          0.02702714,  0.02702636,  0.0270267 ,  0.02702731,  0.02702636,\n",
       "          0.02702722,  0.02702676,  0.02702632,  0.02702713,  0.0270272 ,\n",
       "          0.02702673,  0.02702623,  0.02702795,  0.0270278 ,  0.02702839,\n",
       "          0.02702634,  0.02702679,  0.02702616,  0.02702764,  0.02702781,\n",
       "          0.02702767,  0.02702656,  0.02702649,  0.02702638,  0.02702778,\n",
       "          0.02702733,  0.02702661]], dtype=float32),\n",
       " array([[ 0.02702705,  0.02702669,  0.02702749,  0.02702726,  0.02702688,\n",
       "          0.02702712,  0.02702669,  0.02702715,  0.02702647,  0.02702775,\n",
       "          0.0270274 ,  0.02702808,  0.02702668,  0.027028  ,  0.02702793,\n",
       "          0.02702729,  0.02702699,  0.02702791,  0.02702617,  0.02702678,\n",
       "          0.02702667,  0.02702682,  0.02702667,  0.02702612,  0.02702774,\n",
       "          0.02702753,  0.02702639,  0.02702722,  0.02702631,  0.02702639,\n",
       "          0.02702701,  0.0270264 ,  0.0270271 ,  0.02702656,  0.02702686,\n",
       "          0.02702718,  0.02702737]], dtype=float32),\n",
       " array([[ 0.02702771,  0.02702707,  0.02702775,  0.02702682,  0.02702637,\n",
       "          0.02702734,  0.02702705,  0.02702592,  0.02702752,  0.02702671,\n",
       "          0.02702683,  0.02702718,  0.02702671,  0.02702692,  0.02702726,\n",
       "          0.02702797,  0.02702829,  0.02702675,  0.02702782,  0.02702579,\n",
       "          0.02702717,  0.02702729,  0.02702642,  0.02702727,  0.02702774,\n",
       "          0.02702613,  0.02702713,  0.02702758,  0.02702771,  0.02702725,\n",
       "          0.02702727,  0.02702689,  0.02702665,  0.02702546,  0.02702789,\n",
       "          0.0270262 ,  0.02702615]], dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"data.npy\")\n",
    "X = data[()][\"X\"]\n",
    "y = data[()][\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 100, 1)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.argmax(y_pred,axis=-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 23)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_word(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.849"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "for i,label in enumerate(labels[:1000]):\n",
    "    score += labels_to_word(test[i])==label\n",
    "score/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interrupts'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_word(np.argmax(y_pred,axis=-1).reshape((1,23))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1686529b0>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACOCAYAAAAhHfOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16115d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[12][:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pynlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-daf1f7305ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpynlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pynlp'"
     ]
    }
   ],
   "source": [
    "import pynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
